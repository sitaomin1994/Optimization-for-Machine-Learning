{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1) #reproducible\n",
    "EPOCH = 4\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist', #保存位置\n",
    "    train=True, #training set\n",
    "    transform=torchvision.transforms.ToTensor(), #converts a PIL.Image to torch.FloatTensor(C*H*W) in range(0.0,1.0)\n",
    "    download=True\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./MNIST',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(learning_rate,u0,idx):\n",
    "    torch.manual_seed(27)\n",
    "    # network structure\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self,D_in,H,D_out):\n",
    "            super(CNN, self).__init__()\n",
    "            self.fc1 = nn.Linear(D_in,H)\n",
    "            torch.nn.init.normal(self.fc1.weight, mean=0, std=0.03)\n",
    "            #nn.init.xavier_normal(self.fc1.weight,gain = 1)\n",
    "            nn.init.constant(self.fc1.bias, 0.1)\n",
    "        \n",
    "            self.fc2 = nn.Linear(H,D_out)\n",
    "            torch.nn.init.normal(self.fc2.weight, mean=0, std=0.3)\n",
    "            #nn.init.xavier_normal(self.fc2.weight, gain = 1)\n",
    "            nn.init.constant(self.fc2.bias, 0.1)\n",
    "           # self.out = nn.Linear(10,10)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, 784)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.softmax(self.fc2(x))\n",
    "            output = x\n",
    "            #output = self.out(x)\n",
    "            return output\n",
    "\n",
    "    D_in,H,D_out = 784,10,10\n",
    "    cnn = CNN(D_in,H,D_out)\n",
    "\n",
    "    # initial hyperpaameter\n",
    "    #loss function:cross-entropy with l2 regularizaiton\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    # inital all using viariables\n",
    "    EMAg = []\n",
    "    EMAg_2 = []\n",
    "    EMAx = []\n",
    "    EMAx_2 = []\n",
    "    EMAxg = []\n",
    "    EMAu = []\n",
    "    beta = []\n",
    "\n",
    "    # training iteration\n",
    "    for epoch in range(EPOCH):\n",
    "        running_loss = 0.0                        # loss to show\n",
    "        #training each mini-batch in dataloader\n",
    "        for i, data in enumerate(train_loader,0):\n",
    "           # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            cnn.zero_grad()\n",
    "\n",
    "            # forward + backward\n",
    "            outputs = cnn(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "       \n",
    "            # implementation of cSGD algo\n",
    "            for index,params in enumerate(cnn.parameters(),0):\n",
    "            \n",
    "                #update parameters process\n",
    "                if(i==0 and epoch==0):\n",
    "                    #lists store EMA of weight_grad, weight_grad_square, weight, and weight_square\n",
    "                \n",
    "                    EMAg.append(torch.Tensor(params.size()))\n",
    "                    EMAx.append(torch.Tensor(params.size()))\n",
    "                    EMAx_2.append(torch.Tensor(params.size()))\n",
    "                    EMAg_2.append(torch.Tensor(params.size()))\n",
    "                    EMAxg.append(torch.Tensor(params.size()))\n",
    "                \n",
    "                    n = params.dim()\n",
    "                    if n ==1:\n",
    "                        for item in range(0,params.size()[0]):\n",
    "                            EMAg[index][item] = params.grad.data[item]\n",
    "                            EMAx[index][item] = params.data[item]\n",
    "                            EMAg_2[index][item] = params.grad.data[item]**2\n",
    "                            EMAx_2[index][item] = params.data[item]**2\n",
    "                            EMAxg[index][item] = params.data[item]*params.grad.data[item]**2\n",
    "                    \n",
    "                    if n ==2:\n",
    "                    \n",
    "                        for item1 in range(0,params.size()[0]):\n",
    "                            for item2 in range(0,params.size()[1]):\n",
    "                                EMAg[index][item1,item2] = params.grad.data[item1,item2]\n",
    "                                EMAx[index][item1,item2] = params.data[item1,item2]\n",
    "                                EMAg_2[index][item1,item2] = params.grad.data[item1,item2]**2\n",
    "                                EMAx_2[index][item1,item2] = params.data[item1,item2]**2\n",
    "                                EMAxg[index][item1,item2] = params.data[item1,item2]*params.grad.data[item1,item2]\n",
    "                \n",
    "                    #lists store EMA of u and beta\n",
    "                    EMAu.append(torch.Tensor(params.size()))\n",
    "                    beta.append(torch.ones(params.size())*0.9)\n",
    "                    EMAu[index] = torch.ones(EMAu[index].size())*u0\n",
    "                    #print(EMAg[index][0,64])\n",
    "                    #print(EMAg_2[index][0,1])\n",
    "                else:\n",
    "                    one = torch.ones(params.size())\n",
    "                    #print(EMAg_2[index][0,1])\n",
    "                    #update EMA\n",
    "                    EMAg[index] = (beta[index])*EMAg[index]+(one-beta[index])*params.grad.data\n",
    "                    EMAg_2[index] = (beta[index])*EMAg_2[index] + (one-beta[index])*(params.grad.data.pow(2))\n",
    "                    EMAx[index] = (beta[index])*EMAx[index] + (one-beta[index])*params.data\n",
    "                    EMAx_2[index] = (beta[index])*EMAx_2[index]+(one-beta[index])*(params.data.pow(2))\n",
    "                    EMAxg[index] = (beta[index])*EMAxg[index] + (one-beta[index])*(params.grad.data*params.data)\n",
    "                \n",
    "                    #print(params.grad.data[0,64])\n",
    "                \n",
    "                    #cal a,b,sigma,u*\n",
    "                    n = params.dim()\n",
    "                    a = torch.Tensor(params.size())\n",
    "                    b = torch.Tensor(params.size())\n",
    "                    sigma = torch.Tensor(params.size())\n",
    "                    u = torch.Tensor(params.size())\n",
    "            \n",
    "            \n",
    "                    if n == 1:\n",
    "                        for item in range(0,params.size()[0]):\n",
    "                            #cal a\n",
    "                            if EMAxg[index][item]==EMAg[index][item]*EMAx[index][item]:\n",
    "                                 a[item] = 0\n",
    "                            elif (EMAx_2[index][item]-EMAx[index][item]**2)==0:\n",
    "                                 a[item] = 0\n",
    "                            else:\n",
    "                                 a[item] = (EMAxg[index][item]-EMAg[index][item]*EMAx[index][item])/(EMAx_2[index][item]-EMAx[index][item]**2)\n",
    "                        \n",
    "                        \n",
    "                            #cal sigma\n",
    "                            sigma[item] = EMAg_2[index][item] - EMAg[index][item]**2\n",
    "                        \n",
    "                            #cal u*\n",
    "                            if(a[item]<= 0):\n",
    "                                u[item] = 1\n",
    "                            else:\n",
    "                                if(EMAg[index][item]==0):\n",
    "                                    u[item] = 0.0\n",
    "                                elif(sigma[item]==0 or a[item]==0):\n",
    "                                    u[item] = 1.0\n",
    "                                else:\n",
    "                                    u[item] = min(1,(EMAg[index][item]**2)/(learning_rate*sigma[item]*a[item])) \n",
    "                            #update EMA u\n",
    "                            EMAu[index][item] = (1-beta[index][item])*EMAu[index][item] + beta[index][item]*u[item]\n",
    "                    \n",
    "                            #cal beta\n",
    "                            if (EMAg_2[index][item]==EMAg[index][item]**2):\n",
    "                                beta[index][item] = 0.9\n",
    "                            elif (EMAg_2[index][item]==0):\n",
    "                                beta[index][item] = 0.9\n",
    "                            else:\n",
    "                                beta[index][item] = 0.9+(0.999-0.9)*(EMAg_2[index][item]-EMAg[index][item]**2)/(EMAg_2[index][item])\n",
    "                        \n",
    "                    if n == 2:\n",
    "                    \n",
    "                        for item1 in range(0,params.size()[0]):\n",
    "                            for item2 in range(0,params.size()[1]):\n",
    "                                #cal a\n",
    "                                if ((EMAxg[index][item1,item2]-EMAg[index][item1,item2]*EMAx[index][item1,item2])==0):\n",
    "                                    a[item1,item2] = 0\n",
    "                                elif (EMAx_2[index][item1,item2]-EMAx[index][item1,item2]**2)==0:\n",
    "                                    a[item1,item2] = 0\n",
    "                                else:\n",
    "                                    a[item1,item2] =(EMAxg[index][item1,item2]-EMAg[index][item1,item2]*EMAx[index][item1,item2])/(EMAx_2[index][item1,item2]-EMAx[index][item1,item2]**2)            \n",
    "                                    #print(a[item1,item2])\n",
    "                        \n",
    "                                #cal sigma\n",
    "                                sigma[item1,item2] = EMAg_2[index][item1,item2] - math.pow(EMAg[index][item1,item2],2)\n",
    "                                #print(sigma[item1,item2])\n",
    "                                #if sigma[item1,item2]<0 :\n",
    "                                #    print(\"%d %d %d\"%(i,item1,item2))\n",
    "                                #    print(EMAg_2[index][item1,item2],EMAg[index][item1,item2])\n",
    "                                #cal u*\n",
    "                                if(a[item1,item2]<= 0):\n",
    "                                    u[item1,item2] = 1.0\n",
    "                                else:\n",
    "                                    if(EMAg[index][item1,item2]==0):\n",
    "                                        u[item1,item2] = 0.0\n",
    "                                    elif(sigma[item1,item2]==0 or a[item1,item2]==0):\n",
    "                                        u[item1,item2] = 1.0\n",
    "                                    else:\n",
    "                                        u[item1,item2] = min(1,(EMAg[index][item1,item2]**2)/(learning_rate*sigma[item1,item2]*a[item1,item2])) \n",
    "                                        #print(sigma[item1,item2]*a[item1,item2],EMAg[index][item1,item2]**2)\n",
    "                                        #print(EMAg[index][item1,item2]**2)\n",
    "                                        #print(a[item1,item2]*((EMAx[index][item1,item2]-b[item1,item2])**2)/(learning_rate*sigma[item1,item2]))\n",
    "                                #update EMAu\n",
    "                                EMAu[index][item1,item2] = (1-beta[index][item1,item2])*EMAu[index][item1,item2] + (beta[index][item1,item2])*u[item1,item2]\n",
    "                                #print(params.grad.data[5,5],params.data[5,5])\n",
    "                                #print(EMAx[index][0,5],EMAg[index][0,5],EMAu[index][0,5])\n",
    "                            \n",
    "                                #if i>=100:\n",
    "                                #    print(EMAu[index][item1,item2])\n",
    "                                #cal beta\n",
    "                                if EMAg_2[index][item1,item2]==EMAg[index][item1,item2]**2:\n",
    "                                    beta[index][item1,item2] = 0.9\n",
    "                                elif (EMAg_2[index][item1,item2]==0):\n",
    "                                    beta[index][item1,item2] = 0.9\n",
    "                                else:\n",
    "                                    beta[index][item1,item2] = 0.9+(0.999-0.9)*(EMAg_2[index][item1,item2]-EMAg[index][item1,item2]**2)/(EMAg_2[index][item1,item2])\n",
    "                        #print(params.grad.data[5,5],params.data[5,5],EMAu[index][5,5])\n",
    "                        #print(EMAx[index][5,5],EMAg[index][5,5],EMAu[index][5,5])\n",
    "                #update weight and bias\n",
    "            \n",
    "                params.data -= learning_rate *EMAu[index]* params.grad.data\n",
    "        \n",
    "            #print(\"%d %d\"%(epoch, i))\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 100 == 0:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.8f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "                correct = 0.0\n",
    "                total = 0.0\n",
    "                for data in test_loader:\n",
    "                    images, labels = data\n",
    "                    outputs = cnn(Variable(images))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum()\n",
    "                    #result_data[idx][i/100+epoch*4] = correct/total\n",
    "                result_data1[idx][int(i/100)+epoch*5] = correct/total\n",
    "    print('Finished Training')\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = cnn(Variable(images))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('lr:%f u0:%f Accuracy: %f' % (learning_rate,u0,(correct / total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.36821994\n",
      "[1,   200] loss: 2.36285792\n",
      "[1,   300] loss: 2.33817821\n",
      "[1,   400] loss: 2.32242626\n",
      "[2,   100] loss: 2.26988127\n",
      "[2,   200] loss: 2.25620906\n",
      "[2,   300] loss: 2.24440068\n",
      "[2,   400] loss: 2.19782070\n",
      "[3,   100] loss: 2.12975661\n",
      "[3,   200] loss: 2.11873385\n",
      "[3,   300] loss: 2.10838831\n",
      "[3,   400] loss: 2.10146113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d212ca0f8540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#beta_w2 = 1/(10*10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#beta_b = 1/(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0e951946ed34>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(learning_rate, u0, idx)\u001b[0m\n\u001b[1;32m    184\u001b[0m                                         \u001b[0;31m#print(a[item1,item2]*((EMAx[index][item1,item2]-b[item1,item2])**2)/(learning_rate*sigma[item1,item2]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                                 \u001b[0;31m#update EMAu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                                 \u001b[0mEMAu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mEMAu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                                 \u001b[0;31m#print(params.grad.data[5,5],params.data[5,5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                                 \u001b[0;31m#print(EMAx[index][0,5],EMAg[index][0,5],EMAu[index][0,5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(8)\n",
    "\n",
    "#random_u0 = -2*np.random.random(20)\n",
    "#random_lr = -1*np.random.random(20)\n",
    "x_u0 = [0.2,0.01]\n",
    "#x_u0 = np.sort(10**random_u0)\n",
    "x_lr = [0.2,0.6]\n",
    "result_data = np.zeros((len(x_u0),20))\n",
    "result_data1 = np.zeros((len(x_u0),20))\n",
    "#x_lr = np.sort(10**random_lr)\n",
    "#x = [0.002,0.04,0.3]\n",
    "for index in range(0, len(x_u0)):\n",
    "    lr = x_lr[index]\n",
    "    u0 = x_u0[index]\n",
    "    #beta_w1 = 1/(784*10)\n",
    "    #beta_w2 = 1/(10*10)\n",
    "    #beta_b = 1/(10)\n",
    "    train(lr,u0,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.387 ,  0.7292,  0.8143,  0.8291,  0.835 ,  0.8409,  0.8442,\n",
       "        0.84  ,  0.8469,  0.8489,  0.849 ,  0.8512,  0.852 ,  0.8512,\n",
       "        0.8527,  0.8518])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_9_1 = np.zeros(16)\n",
    "result_9_1 = result_data1[0,:]\n",
    "result_9_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 = result_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Xmax = np.zeros(43)\n",
    "Xmin = np.zeros(43)\n",
    "X_lrmax = np.zeros(43)\n",
    "X_lrmin = np.zeros(43)\n",
    "X_lrmedian = np.zeros(43)\n",
    "Xlr = y\n",
    "Xsum = np.zeros(30)\n",
    "\n",
    "# find max and min each test step to get area of accuracy distribution\n",
    "for n in range(0,43):\n",
    "    Xmax[n] = np.max(result_data[:,n])\n",
    "    Xmin[n] = np.min(result_data[:,n])\n",
    "\n",
    "# calculate area-under-curve and save it to Xsum\n",
    "for m in range(0,30):   \n",
    "    Xsum[m]  = np.sum(result_data[m,:])\n",
    "\n",
    "# find index of learning rate refer to max and min auc \n",
    "lr_min_i = np.argwhere(Xsum == np.min(Xsum))\n",
    "lr_max_i = np.argwhere(Xsum == np.max(Xsum))\n",
    "\n",
    "lr_min = Xlr[lr_min_i[0,0]]\n",
    "lr_max = Xlr[lr_max_i[0,0]]\n",
    "\n",
    "\n",
    "for n in range(0,43):\n",
    "    X_lrmin[n] = result_data[lr_min_i[0,0],n]\n",
    "    X_lrmax[n] = result_data[lr_max_i[0,0],n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib.ticker import  MultipleLocator\n",
    "matplotlib.use('Agg')\n",
    "x_index=np.arange(0,4,4.0/43)\n",
    "X_lrmedian = result_data[13,:]\n",
    "fig = plt.figure()\n",
    "plt.title(\"cSGD\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accu')\n",
    "plt.axis([0,4,0.0,1.0]) \n",
    "plt.plot(x_index,X_lrmedian,color = 'red',label='1e-2')\n",
    "plt.plot(x_index,X_lrmin,'r--',label='1e-3')\n",
    "plt.plot(x_index,X_lrmax,'r-.',label='2e-1')\n",
    "plt.fill_between(x_index,Xmin,Xmax,color = 'red',alpha = '0.1')\n",
    "plt.legend(loc='lower right',framealpha = 0.5)\n",
    "plt.savefig(\"cSGD new.png\") \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "adagrad_result_csv = pd.DataFrame(np.zeros((30,43)))\n",
    "adagrad_result_csv.iloc[:,:] = result_data\n",
    "adagrad_result_csv.to_csv('cSGD',index=False)\n",
    "training_csv = pd.read_csv('cSGD',index_col=0)\n",
    "training_csv\n",
    "\n",
    "np.save('cSGD-np.npy',result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib.ticker import  MultipleLocator\n",
    "matplotlib.use('Agg')\n",
    "x_index=np.arange(0,4,4.0/43)\n",
    "X1 = result_data1[0,:]\n",
    "X2 = result_data1[1,:]\n",
    "X3 = result_data1[2,:]\n",
    "fig = plt.figure()\n",
    "plt.title(\"cSGD\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accu')\n",
    "plt.axis([0,4,0.0,1.0]) \n",
    "plt.plot(x_index,X1,'r--',label='2e-3')\n",
    "plt.plot(x_index,X2,color = 'red',label='4e-2')\n",
    "plt.plot(x_index,X3,'r-.',label='3e-1')\n",
    "plt.fill_between(x_index,X1,X3,color = 'red',alpha = '0.1')\n",
    "plt.legend(loc='lower right',framealpha = 0.5)\n",
    "plt.savefig(\"cSGD para.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda/lib/python3.6/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f08c320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib \n",
    "from matplotlib.ticker import  MultipleLocator\n",
    "matplotlib.use('Agg')\n",
    "x_index=np.arange(0,4,4.0/16)\n",
    "X1 = result_9_1\n",
    "#X2 = result_data1[1,:]\n",
    "#X3 = result_data1[2,:]\n",
    "fig = plt.figure()\n",
    "plt.title(\"cSGD\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accu')\n",
    "plt.axis([-0.1,4,0.0,1.0]) \n",
    "plt.plot(x_index,X1,'r--',label='9e-1')\n",
    "#plt.plot(x_index,X2,color = 'red',label='4e-2')\n",
    "#plt.plot(x_index,X3,'r-.',label='3e-1')\n",
    "#plt.fill_between(x_index,X1,X3,color = 'red',alpha = '0.1')\n",
    "plt.legend(loc='lower right',framealpha = 0.5)\n",
    "#plt.savefig(\"cSGD para.png\") \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
